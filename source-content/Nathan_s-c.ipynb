{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\Anaconda\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/source-content/parc_features/parc_train_features.tsv\", sep=\"\\t\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POS', 'dependency_head', 'dependency_label', 'doc_token_number',\n",
       "       'lemma', 'ne_info', 'sentence_number', 'sentence_token_number', 'token',\n",
       "       'cue_label', 'attribution', 'token_-1', 'token_-2', 'token_-3',\n",
       "       'token_-4', 'token_-5', 'token_+1', 'token_+2', 'token_+3', 'token_+4',\n",
       "       'token_+5', 'lemma_-1', 'lemma_-2', 'lemma_-3', 'lemma_-4', 'lemma_-5',\n",
       "       'lemma_+1', 'lemma_+2', 'lemma_+3', 'lemma_+4', 'lemma_+5', 'POS_-1',\n",
       "       'POS_-2', 'POS_-3', 'POS_-4', 'POS_-5', 'POS_+1', 'POS_+2', 'POS_+3',\n",
       "       'POS_+4', 'POS_+5', 'bigram_prev_token', 'bigram_prev_lemma',\n",
       "       'bigram_prev_POS', 'bigram_following_token', 'bigram_following_lemma',\n",
       "       'bigram_following_POS', 'shape', 'ne_short', 'relevant_ne', 'ne_+-5',\n",
       "       'candidate_cue', 'reporting_verb', 'quotation', 'near_sent_boundary',\n",
       "       'near_doc_boundary', 'dist_beg_sent', 'dist_end_sent', 'sent_len',\n",
       "       'pn_in_sent', 'ne_in_sent', 'qm_in_sent', 'any_in_sent', 'quotation_pn',\n",
       "       'quotation_ne', 'quotation_qm', 'filename', 'content_label_gold',\n",
       "       'ancestor_is_cue', 'cue_in_window_of_5', 'distance_to_prev_cue',\n",
       "       'distance_to_next_cue', 'cue_in_sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Analysis of Sources: Are they NEs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = defaultdict(int)\n",
    "count = 0\n",
    "for token, pos, attribution, relevant_ne in zip(df[\"token\"], df[\"POS\"], df[\"attribution\"], df[\"relevant_ne\"]):\n",
    "    source = False\n",
    "    att_list = attribution.split(\" \")\n",
    "    for att in att_list:\n",
    "        att_split = att.split(\"-\")\n",
    "        if att_split[0] not in {\"_\", \"0\", \"\"} and att_split[1] == \"SOURCE\":\n",
    "            count +=1\n",
    "            if relevant_ne == 0:\n",
    "                pos_dict[pos] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_count = 0\n",
    "non_ne = 0\n",
    "for token, pos, attribution, ne_info in zip(df[\"token\"], df[\"POS\"], df[\"attribution\"], df[\"ne_info\"]):\n",
    "    source = False\n",
    "    att_list = attribution.split(\" \")\n",
    "    for att in att_list:\n",
    "        att_split = att.split(\"-\")\n",
    "        if att_split[0] not in {\"_\", \"0\", \"\"} and att_split[1] == \"SOURCE\":\n",
    "            if ne_info != \"O\":\n",
    "                ne_count +=1\n",
    "            else:\n",
    "                non_ne += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "\n",
    "Note that this is all at the token level - so when I say \"number of sources\" I mean \"number of tokens that appear in source spans\"\n",
    "\n",
    "pos_dict is a dict of the POS of sources that are NOT relevant NEs.\n",
    "\n",
    "count is the total number of sources.\n",
    "\n",
    "ne_count is the number of sources that are NEs (both relevant and other).\n",
    "\n",
    "non_ne is the number of sources that are NOT NEs of any type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sources that are not relevant NEs: 37777\n",
      "Total sources: 60513\n",
      "Total sources that are any type of NE: 23845\n",
      "Total sources that are NOT any type of NE: 36668\n",
      "Total sources that are pronouns: 3570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sources that are not relevant NEs: {sum(pos_dict.values())}\")\n",
    "print(f\"Total sources: {count}\")\n",
    "print(f\"Total sources that are any type of NE: {ne_count}\")\n",
    "print(f\"Total sources that are NOT any type of NE: {non_ne}\")\n",
    "print(f\"Total sources that are pronouns: {pos_dict['PRP']+pos_dict['PRP$']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'NNS': 3425,\n",
       "             'DT': 5403,\n",
       "             'NN': 8608,\n",
       "             'PRP': 3406,\n",
       "             'IN': 3491,\n",
       "             'POS': 550,\n",
       "             'NNP': 3010,\n",
       "             ',': 4077,\n",
       "             'JJ': 2224,\n",
       "             'JJR': 40,\n",
       "             'RB': 216,\n",
       "             'VBN': 298,\n",
       "             'WDT': 225,\n",
       "             'CC': 625,\n",
       "             'CD': 536,\n",
       "             'VBG': 213,\n",
       "             'VBZ': 222,\n",
       "             'WP': 368,\n",
       "             'VBD': 171,\n",
       "             'VBP': 52,\n",
       "             'PRP$': 164,\n",
       "             'JJS': 86,\n",
       "             'NNPS': 58,\n",
       "             '``': 21,\n",
       "             \"''\": 27,\n",
       "             'TO': 47,\n",
       "             'VB': 70,\n",
       "             '$': 35,\n",
       "             'RBR': 6,\n",
       "             'RBS': 21,\n",
       "             '.': 10,\n",
       "             'WRB': 11,\n",
       "             'PDT': 13,\n",
       "             'MD': 16,\n",
       "             'WP$': 17,\n",
       "             ':': 1,\n",
       "             'HYPH': 2,\n",
       "             'RP': 10,\n",
       "             'UH': 2})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_dict = defaultdict(int)\n",
    "for pos in df[\"POS\"]:\n",
    "    total_pos_dict[pos]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102182"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pos_dict[\"NNP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "It is NOT enough by a long shot to only consider NEs and pronouns as candidate sources (~34000/60513 tokens in source spans, about 55%, are NOT relevant NEs or pronouns.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Candidate Mention Detection\n",
    "This happens at the token/phrase level; ideally we have IOB spans that can be collapsed into symbols in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Content in sentence label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_in_sentence(df):\n",
    "    '''\n",
    "    Takes a df with \"attribution\", \"filename\", and \"sentence_number\" columns and returns a list (column) of binary\n",
    "    \"sentence contains a content\" labels\n",
    "    '''\n",
    "    sent_with_content = set()\n",
    "    for filename, sentence_number, attribution in zip(df[\"filename\"], df[\"sentence_number\"], df[\"attribution\"]):\n",
    "        for att in attribution.split(\" \"):\n",
    "            att_split = att.split(\"-\")\n",
    "            if att_split[0] not in {\"_\", \"0\", \"\"} and att_split[1] == \"CONTENT\":\n",
    "                sent_with_content.add((filename, sentence_number))\n",
    "    labels = []\n",
    "    for filename, sentence_number in zip(df[\"filename\"], df[\"sentence_number\"]):\n",
    "        if (filename, sentence_number) in sent_with_content:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_in_sent_labels = content_in_sentence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_in_sent\"] = content_in_sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: count sources in content sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_in_sent_count = 0\n",
    "for token, pos, attribution, content_in_sent_label in zip(df[\"token\"], df[\"POS\"], df[\"attribution\"], df[\"content_in_sent\"]):\n",
    "    source = False\n",
    "    att_list = attribution.split(\" \")\n",
    "    for att in att_list:\n",
    "        att_split = att.split(\"-\")\n",
    "        if att_split[0] not in {\"_\", \"0\", \"\"} and att_split[1] == \"SOURCE\":\n",
    "            if content_in_sent_label == 1:\n",
    "                content_in_sent_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources that occur in sentences that contain contents: 60274\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sources that occur in sentences that contain contents: {content_in_sent_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion here: Basically all (60274/60513) sources appear in the same sentence as a content\n",
    "Note that this doesn't represent sources that appear in the same sentence as their content, necessarily.\n",
    "\n",
    "It does mean that only considering tokens/spans that appear in the same sentence as a content is a good way to determine candidate sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_candidate_dict = {\"NN\":[0,0], \"NNS\":[0,0], \"NNP\":[0,0], \"PRP\":[0,0]}\n",
    "for token, pos, attribution, label in zip(df[\"token\"], df[\"POS\"], df[\"attribution\"], df[\"content_in_sent\"]):\n",
    "    source = False\n",
    "    att_list = attribution.split(\" \")\n",
    "    if pos in {\"NN\", \"NNS\", \"NNP\", \"PRP\"} and label == 1:\n",
    "        source = False\n",
    "        for att in att_list:\n",
    "            att_split = att.split(\"-\")\n",
    "            if att_split[0] not in {\"_\", \"0\", \"\"} and att_split[1] == \"SOURCE\":\n",
    "                pos_candidate_dict[pos][0] +=1\n",
    "                source = True\n",
    "        pos_candidate_dict[pos][1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary represents the occurrence of POS's in sentences with contents.\n",
    "The first number is a count of the POS AS A SOURCE in such sentences, and the second is the total count of the POS in such sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': [8810, 61333],\n",
       " 'NNS': [3492, 26404],\n",
       " 'NNP': [22681, 42763],\n",
       " 'PRP': [3402, 11974]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_candidate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion here: Probably viable to use these POS in sentences with content as candidate mentions; we'll have a ~25% positive example ratio, and we should get nearly all mentions.\n",
    "\n",
    "The next valuable step would be to determine the number of sources covered at least partially (as in, not necessarily whole span) by such POS's. The numbers to this point are entirely token-based (not overall source span based)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Collapsing/Anonymizing Contents and Sources\n",
    "The goal here is to take content and source spans and convert them into symbols.\n",
    "\n",
    "The complication is that we need to keep working with DFs, and it'll be hard to collapse portions of a column or two and maintain the other columns.\n",
    "\n",
    "I need to decide what info needs to come out of this. That comes down to two things: i) the rest of this classifier (what is needed for the feature engineering?), and ii) evaluation: we can evaluate based on the labels we concoct, but what about at the level of the source-contents we're provided? Ideally we'll use the eval script from Roser, but that makes this conversion too and from our classifier DF really challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, there's a pandas functionality that lets me preserve index nums as I collapse the DF so they can be put back.\n",
    "\n",
    "There is, of course, the \"filename\" and \"doc_token_number\" columns to work with; in converting back to eval mode these can be all the difference.\n",
    "\n",
    "The other question is how do I deal with multi-part contents/spans? Where do I put them, and how on Earth do I convert them back? I probably need to keep information on the original location of these spans before I collapse them entirely. Maybe a \"CONTENT-(span1)\\_(span2)\\_...\" kind of label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
